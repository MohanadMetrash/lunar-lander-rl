# -*- coding: utf-8 -*-
"""Lunar_Lander_RL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DVVHtj4e_7K1lcvEsA8qlq_A_TH2VSdr

## Install dependencies and create a virtual screen ðŸ”½
"""

!apt install swig cmake

!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt

!sudo apt-get update
!sudo apt-get install -y python3-opengl
!apt install ffmpeg
!apt install xvfb
!pip3 install pyvirtualdisplay

!pip install huggingface_sb3 stable_baselines3 gymnasium[box2d]

# Virtual display
from pyvirtualdisplay import Display

virtual_display = Display(visible=0, size=(1400, 900))
virtual_display.start()

"""## Import the packages ðŸ“¦

"""

import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt
import os

from stable_baselines3 import PPO, DQN
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.callbacks import EvalCallback
from stable_baselines3.common.results_plotter import load_results, ts2xy

# For video recording and display
from gymnasium.wrappers import RecordVideo
from IPython.display import Video, display, FileLink

"""## Create Environment"""

# Create directories to save logs and models
log_dir = "logs/"
ppo_log_dir = os.path.join(log_dir, "ppo/")
dqn_log_dir = os.path.join(log_dir, "dqn/")
os.makedirs(ppo_log_dir, exist_ok=True)
os.makedirs(dqn_log_dir, exist_ok=True)

"""## USING PPO"""

print("\n--- Training PPO Model ---")

# Create a training environment that saves logs to the correct directory
train_env_ppo = make_vec_env('LunarLander-v3', n_envs=16, monitor_dir=ppo_log_dir)

# Create a separate, non-vectorized environment for evaluation
eval_env_ppo = Monitor(gym.make("LunarLander-v3"))

# Use an EvalCallback to save the best model during training
eval_callback_ppo = EvalCallback(eval_env_ppo, best_model_save_path=ppo_log_dir,
                                 log_path=ppo_log_dir, eval_freq=5000,
                                 deterministic=True, render=False)

# Tuned hyperparameters for PPO
model_PPO = PPO(
    policy='MlpPolicy',
    env=train_env_ppo, # **FIX:** Use the new environment
    n_steps=1024,
    batch_size=64,
    n_epochs=4,
    gamma=0.999,
    gae_lambda=0.98,
    ent_coef=0.01,
    learning_rate=3e-4,
    verbose=1
)

TOTAL_TIMESTEPS = 2500000
model_PPO.learn(total_timesteps=TOTAL_TIMESTEPS, callback=eval_callback_ppo, progress_bar=True)

# Load the best model
best_model_PPO = PPO.load(os.path.join(ppo_log_dir, "best_model"))

# Evaluate the best PPO model on 100 episodes for a reliable score
mean_reward_PPO, std_reward_PPO = evaluate_policy(best_model_PPO, eval_env_ppo, n_eval_episodes=100, deterministic=True)
print(f"PPO Mean Reward (Best Model) = {mean_reward_PPO:.2f} +/- {std_reward_PPO:.2f}\n")

"""## USING DQN"""

print("--- Training DQN Model ---")

# Create a training environment for DQN that saves logs to its own directory
train_env_dqn = make_vec_env('LunarLander-v3', n_envs=16, monitor_dir=dqn_log_dir)

# Create a separate evaluation environment
eval_env_dqn = Monitor(gym.make("LunarLander-v3"))

# Use an EvalCallback for the DQN model
eval_callback_dqn = EvalCallback(eval_env_dqn, best_model_save_path=dqn_log_dir,
                                 log_path=dqn_log_dir, eval_freq=5000,
                                 deterministic=True, render=False)

# Tuned hyperparameters for DQN
model_DQN = DQN(
    policy='MlpPolicy',
    env=train_env_dqn, # **FIX:** Use the new environment
    learning_rate=6.3e-4,
    buffer_size=50000,
    learning_starts=1000,
    batch_size=128,
    gamma=0.99,
    train_freq=4,
    gradient_steps=1,
    exploration_fraction=0.12,
    exploration_final_eps=0.1,
    verbose=1
)

model_DQN.learn(total_timesteps=TOTAL_TIMESTEPS, callback=eval_callback_dqn, progress_bar=True)

# Load the best model
best_model_DQN = DQN.load(os.path.join(dqn_log_dir, "best_model"))

# Evaluate the best DQN model
mean_reward_DQN, std_reward_DQN = evaluate_policy(best_model_DQN, eval_env_dqn, n_eval_episodes=100, deterministic=True)
print(f"DQN Mean Reward (Best Model) = {mean_reward_DQN:.2f} +/- {std_reward_DQN:.2f}\n")

"""## RECORD AND DISPLAY VIDEOS OF THE BEST MODELS"""

# --- PPO Video ---
print("--- Recording video of the best PPO agent ---")
video_folder_ppo = 'videos/ppo/'
video_length = 1000
eval_env_ppo_video = RecordVideo(
    gym.make("LunarLander-v3", render_mode='rgb_array'),
    video_folder=video_folder_ppo,
    name_prefix="best-ppo-agent",
    episode_trigger=lambda x: x == 0,
    disable_logger=True
)
obs, _ = eval_env_ppo_video.reset()
for _ in range(video_length):
    action, _ = best_model_PPO.predict(obs, deterministic=True)
    obs, _, terminated, truncated, _ = eval_env_ppo_video.step(action)
    if terminated or truncated:
        break
eval_env_ppo_video.close()
ppo_video_files = [f for f in os.listdir(video_folder_ppo) if f.endswith('.mp4')]
if ppo_video_files:
    video_path = os.path.join(video_folder_ppo, ppo_video_files[0])
    print(f"Displaying PPO video: {video_path}")
    display(Video(video_path, embed=True))
else:
    print(f"No PPO video found in {video_folder_ppo}")

# --- DQN Video ---
print("\n--- Recording video of the best DQN agent ---")
video_folder_dqn = 'videos/dqn/'
eval_env_dqn_video = RecordVideo(
    gym.make("LunarLander-v3", render_mode='rgb_array'),
    video_folder=video_folder_dqn,
    name_prefix="best-dqn-agent",
    episode_trigger=lambda x: x == 0,
    disable_logger=True
)
obs, _ = eval_env_dqn_video.reset()
for _ in range(video_length):
    action, _ = best_model_DQN.predict(obs, deterministic=True)
    obs, _, terminated, truncated, _ = eval_env_dqn_video.step(action)
    if terminated or truncated:
        break
eval_env_dqn_video.close()
dqn_video_files = [f for f in os.listdir(video_folder_dqn) if f.endswith('.mp4')]
if dqn_video_files:
    video_path = os.path.join(video_folder_dqn, dqn_video_files[0])
    print(f"Displaying DQN video: {video_path}")
    display(Video(video_path, embed=True))
else:
    print(f"No DQN video found in {video_folder_dqn}")

"""## VISUALIZE PERFORMANCE"""

def plot_learning_curves(log_folder_ppo, log_folder_dqn, title='Learning Curve'):
    """ Plots the learning curves from the log files of two models. """
    # Load and smooth PPO results
    x_ppo, y_ppo = ts2xy(load_results(log_folder_ppo), 'timesteps')
    y_ppo_smooth = np.convolve(y_ppo, np.ones(50)/50, mode='valid')
    x_ppo_smooth = x_ppo[len(x_ppo) - len(y_ppo_smooth):]

    # Load and smooth DQN results
    x_dqn, y_dqn = ts2xy(load_results(log_folder_dqn), 'timesteps')
    y_dqn_smooth = np.convolve(y_dqn, np.ones(50)/50, mode='valid')
    x_dqn_smooth = x_dqn[len(x_dqn) - len(y_dqn_smooth):]

    plt.figure(title, figsize=(12, 7))
    plt.plot(x_ppo_smooth, y_ppo_smooth, color='b', label='PPO')
    plt.plot(x_dqn_smooth, y_dqn_smooth, color='r', label='DQN')
    plt.xlabel('Number of Timesteps')
    plt.ylabel('Mean Reward')
    plt.title(title)
    plt.grid(True)
    plt.legend()
    plt.show()

print("\n--- Plotting Learning Curves ---")
plot_learning_curves(ppo_log_dir, dqn_log_dir, "PPO vs. DQN on LunarLander-v3")